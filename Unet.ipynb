{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ee52967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65f22755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_sc(tensor, target_tensor): \n",
    "#crop the skip connection to size of upsample target \n",
    "#tensor -> upsample\n",
    "#tensor -> skip connection\n",
    "    tensor = TF.resize(tensor, size=target_tensor.shape[2:]) \n",
    "    return tensor\n",
    "\n",
    "'''\n",
    "The double convolution function gets as input the number of \n",
    "input channels and output channels. For the first layer this \n",
    "will be 1 and 64 because the input image is black and white.\n",
    "Note that if you had an RGB image the input channels would be 3.\n",
    "'''\n",
    "\n",
    "'''\n",
    "ReLU is an activation function, F(x) = max(0,x) \n",
    "if x =< 0 then f(x) = 0 \n",
    "if x > 0 then f(x) = x\n",
    "negatives are mapped to 0 therefore It is not a linear function ofc, \n",
    "so introduces non linearity which provides better understanding of the \n",
    "complex features\n",
    "\n",
    "sigmoid maps activations between 0,1 where when the network is large \n",
    "causes vanishing gradients, ReLU solves the vanishing gradients problem\n",
    "'''\n",
    "\n",
    "def double_conv(in_ch, out_ch):\n",
    "#apply 2 convolution operations sequantially        \n",
    "    conv = nn.Sequential(\n",
    "        nn.Conv2d(in_ch, out_ch, 3),\n",
    "        nn.ReLU(inplace = True), \n",
    "            \n",
    "        nn.Conv2d(out_ch, out_ch, 3),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "'''\n",
    "Defining a U-Net class that implements the nn.Module which is \n",
    "used as a base class for all neural network modules in PyTorch. \n",
    "It provides a standard interface for creating and managing the \n",
    "parameters of a module, as well as methods for forwarding input \n",
    "through the module.\n",
    "'''\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \n",
    "    '''\n",
    "Input is an RGB image, output is a segmentation mask consisting of 3 colors\n",
    "Background: black, Obj1: green, Obj2: blue \n",
    "out_channels depends on the number of classes you have in your output mask. \n",
    "Features are the number of filters to be applied at each layer. \n",
    "Number of filters are up to you and to the need of your application. \n",
    "It is common that deeper layers have higher amount of filters bc they extract \n",
    "more meaningful features. \n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels = 3, out_channels = 3, features = [64,128,256,512,1024]):\n",
    "        \n",
    "        super(UNet, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        #Using ModuleList to store Convolutions to be applied at each layer.\n",
    "        \n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        \n",
    "        '''\n",
    "        Save double convolution operations in the downs ModuleList\n",
    "        downs[0]: double_conv(3,64) \n",
    "        downs[1]: double_conv(64,128)\n",
    "        downs[2]: double_conv(128,256) \n",
    "        downs[3]: double_conv(256,512)\n",
    "        downs[4]: double_conv(512,1024)\n",
    "        '''\n",
    "        for feature in features:\n",
    "            self.downs.append(double_conv(in_channels,feature))\n",
    "            in_channels = feature\n",
    "  \n",
    "            \n",
    "        '''\n",
    "        Save transpose convolution operations in the ups ModuleList\n",
    "        after each transpose convolution there is a double convolution applied\n",
    "        ups[0]: ConvTranspose2d(1024,512) \n",
    "        ups[1]: double_conv(1024,512)\n",
    "        ups[2]: ConvTranspose2d(512,256) \n",
    "        ups[3]: double_conv(512,256)\n",
    "        ups[4]: ConvTranspose2d(256,128)\n",
    "        ups[5]: double_conv(256,128) \n",
    "        ups[6]: ConvTranspose2d(128,64)\n",
    "        ups[7]: double_conv(128,64) \n",
    "        '''               \n",
    "        for feature in reversed(features[1:]):\n",
    "            self.ups.append(nn.ConvTranspose2d(feature,feature//2,2,2))\n",
    "            self.ups.append(double_conv(feature,feature//2))\n",
    "            \n",
    "        \n",
    "        #Bottleneck is the last layer before starting the expanding path\n",
    "        #no max pooling is applied after bottleneck, \n",
    "        #so store the operations that will be applied seperately\n",
    "\n",
    "        self.bottleneck = double_conv(features[-1]//2,features[-1]) #512, 1024\n",
    "        self.out = nn.Conv2d(features[0],2,3) #64,2,3\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []  \n",
    "        print(\"x\",x.size())\n",
    "        for down in self.downs[:-1]:\n",
    "            x = down(x)\n",
    "            print(\"before max pool\",x.size())\n",
    "            skip_connections.append(x) #we will use this in concatination with up parts\n",
    "            x = self.max_pool(x)\n",
    "            \n",
    "            \n",
    "        x = self.bottleneck(x) #apply the doubleconv at the bottleneck step, bottleneck ile basla\n",
    "        print(\"convolved bottleneck: \",x.size())\n",
    "        \n",
    "        skip_connections = skip_connections[::-1] #to reverse the list\n",
    "        \n",
    "        for idx in range(0, len(self.ups) , 2):\n",
    "            print(idx)\n",
    "            print(self.ups[idx])\n",
    "            x = self.ups[idx](x) # apply transpose conv, start with bottleneck \n",
    "            s_con = skip_connections[idx//2] #bc idx = 0,2,4,6,8.. get skip connection, skip connection is the last layer form contracting path\n",
    "            print(\"sizes\", x.size(),s_con.size())\n",
    "            #s_con is 64x64x512, 136x136x256, \n",
    "            y = crop_sc(s_con,x) #crop the skip connectio 64 needs to be resized to 56\n",
    "            \n",
    "                #concat\n",
    "            concat = torch.cat([x,y] ,1) #concat bottleneck 512x56x56 with cropped 64 from contracting layer (56x56x512)\n",
    "            ##element wise addition output is 56x56x512\n",
    "            print('doubleconv id',idx+1)\n",
    "            x = self.ups[idx+1](concat) #apply double conv\n",
    "            #double convs are stored in indexes 1,3,5\n",
    "                   \n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a04aeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "#transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "293ef0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([3, 3, 572, 572])\n",
      "before max pool torch.Size([3, 64, 568, 568])\n",
      "before max pool torch.Size([3, 128, 280, 280])\n",
      "before max pool torch.Size([3, 256, 136, 136])\n",
      "before max pool torch.Size([3, 512, 64, 64])\n",
      "convolved bottleneck:  torch.Size([3, 1024, 28, 28])\n",
      "0\n",
      "ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 512, 56, 56]) torch.Size([3, 512, 64, 64])\n",
      "doubleconv id 1\n",
      "2\n",
      "ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 256, 104, 104]) torch.Size([3, 256, 136, 136])\n",
      "doubleconv id 3\n",
      "4\n",
      "ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 128, 200, 200]) torch.Size([3, 128, 280, 280])\n",
      "doubleconv id 5\n",
      "6\n",
      "ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 64, 392, 392]) torch.Size([3, 64, 568, 568])\n",
      "doubleconv id 7\n",
      "tensor([[[[-1.9879e-03, -2.7690e-03, -1.5430e-03,  ..., -1.0176e-03,\n",
      "           -1.2969e-03, -3.3652e-03],\n",
      "          [-3.9479e-04, -6.4619e-04, -3.6455e-04,  ..., -2.4872e-03,\n",
      "           -2.4108e-03,  1.5558e-03],\n",
      "          [-1.0153e-03, -2.4513e-03, -2.6515e-03,  ..., -2.0573e-03,\n",
      "           -1.8142e-03, -3.1323e-03],\n",
      "          ...,\n",
      "          [-9.8476e-04,  3.5266e-05, -3.6472e-03,  ...,  1.1002e-03,\n",
      "           -7.1302e-04, -2.6364e-03],\n",
      "          [-9.5459e-04,  9.5466e-04, -2.8946e-03,  ..., -1.5340e-03,\n",
      "           -5.6577e-04, -2.6412e-04],\n",
      "          [-1.1989e-03, -2.0856e-03,  3.2483e-04,  ..., -1.8725e-03,\n",
      "           -3.9817e-04, -1.0266e-03]],\n",
      "\n",
      "         [[-2.7722e-04, -3.0074e-03, -2.4574e-03,  ..., -4.3077e-03,\n",
      "           -1.3041e-03, -3.5132e-03],\n",
      "          [-1.4768e-03,  1.3089e-03, -1.2727e-03,  ..., -1.2576e-03,\n",
      "           -3.2491e-03, -2.5510e-03],\n",
      "          [-8.2892e-04, -4.9479e-03, -3.4554e-03,  ..., -3.7678e-03,\n",
      "           -9.1411e-05, -2.9921e-03],\n",
      "          ...,\n",
      "          [-2.5194e-03, -1.6279e-03, -1.9788e-03,  ..., -1.5088e-03,\n",
      "           -2.6936e-03, -4.9584e-03],\n",
      "          [-2.0726e-03, -4.1140e-04, -2.1749e-03,  ..., -1.2460e-03,\n",
      "           -3.7546e-03,  2.2978e-04],\n",
      "          [-2.4056e-03, -3.8337e-03, -4.9265e-03,  ..., -2.2670e-04,\n",
      "           -2.8247e-03, -3.1543e-03]]],\n",
      "\n",
      "\n",
      "        [[[-1.3980e-03, -9.5003e-04, -5.6140e-05,  ..., -1.8266e-03,\n",
      "           -1.9671e-03, -3.3388e-03],\n",
      "          [ 1.2121e-03, -3.1035e-04, -2.1877e-03,  ..., -1.4733e-03,\n",
      "           -1.8078e-03, -2.4183e-03],\n",
      "          [-4.7833e-04, -1.7601e-03, -3.8309e-03,  ..., -3.5619e-03,\n",
      "           -3.4045e-03, -2.8980e-03],\n",
      "          ...,\n",
      "          [-2.0324e-03, -1.0126e-03, -2.0065e-03,  ...,  1.3422e-03,\n",
      "           -5.9371e-04, -2.1934e-03],\n",
      "          [-2.0257e-03, -1.5663e-03, -8.4379e-04,  ..., -1.5604e-03,\n",
      "           -1.3633e-03, -7.3448e-04],\n",
      "          [-2.1025e-03, -1.6057e-03, -3.3701e-03,  ..., -4.1624e-04,\n",
      "           -1.6853e-03, -1.9965e-03]],\n",
      "\n",
      "         [[-3.2386e-03, -3.2710e-03, -2.7822e-03,  ..., -2.0139e-03,\n",
      "           -1.4316e-04, -1.8726e-03],\n",
      "          [-2.6218e-03, -1.1521e-03, -9.0323e-04,  ...,  4.9498e-04,\n",
      "            7.2156e-04, -2.0108e-03],\n",
      "          [ 2.0196e-03, -2.1351e-03, -2.9617e-03,  ..., -2.3112e-03,\n",
      "           -4.8529e-03, -3.6211e-03],\n",
      "          ...,\n",
      "          [-3.0343e-03, -7.9660e-04, -3.8885e-03,  ..., -3.6771e-03,\n",
      "           -2.0332e-03, -1.0468e-03],\n",
      "          [-3.0620e-03, -3.7775e-03, -2.3086e-03,  ...,  6.2921e-04,\n",
      "           -3.0790e-03, -1.9756e-03],\n",
      "          [-1.6492e-03, -2.6567e-03, -1.0473e-03,  ..., -5.4886e-03,\n",
      "            9.2893e-05, -5.8005e-04]]],\n",
      "\n",
      "\n",
      "        [[[-2.4952e-03, -2.6306e-03, -9.7828e-04,  ..., -6.0781e-04,\n",
      "           -3.4415e-03, -2.0141e-03],\n",
      "          [-6.3561e-04, -2.0150e-03, -1.3497e-03,  ..., -2.4895e-03,\n",
      "           -1.5161e-03, -2.8469e-03],\n",
      "          [-4.0805e-03, -1.9714e-03, -1.0791e-03,  ..., -2.9097e-03,\n",
      "           -4.1209e-03, -4.9743e-03],\n",
      "          ...,\n",
      "          [-1.0167e-03,  1.7932e-05, -3.2341e-03,  ..., -4.5725e-04,\n",
      "           -7.0332e-04,  6.5005e-04],\n",
      "          [-3.4353e-03, -3.0035e-03, -1.6600e-03,  ..., -2.7596e-04,\n",
      "           -3.4364e-03, -1.9870e-03],\n",
      "          [-9.8682e-04, -3.5394e-03, -3.5477e-03,  ...,  4.7784e-04,\n",
      "           -2.7307e-03, -8.9963e-04]],\n",
      "\n",
      "         [[-1.7785e-03, -2.5711e-03, -3.3766e-03,  ..., -1.9212e-03,\n",
      "           -2.3898e-03, -1.6088e-03],\n",
      "          [-2.8225e-03, -2.3070e-03, -1.2722e-03,  ..., -3.7105e-03,\n",
      "           -3.7489e-03, -1.9922e-03],\n",
      "          [-2.7084e-03, -2.6984e-03, -5.0855e-03,  ..., -1.3868e-03,\n",
      "           -2.4438e-03, -4.9406e-03],\n",
      "          ...,\n",
      "          [-4.0984e-03, -1.9234e-03, -3.4485e-03,  ..., -2.4573e-03,\n",
      "           -3.0541e-03, -1.8284e-03],\n",
      "          [-1.4339e-03, -3.4584e-03, -2.6465e-03,  ..., -2.7719e-03,\n",
      "           -6.9469e-04, -3.7588e-03],\n",
      "          [-1.6466e-03,  5.8725e-06, -2.8865e-03,  ..., -1.2620e-03,\n",
      "           -1.8842e-03, -1.1075e-03]]]], grad_fn=<ConvolutionBackward0>)\n",
      "x torch.Size([3, 3, 572, 572])\n",
      "before max pool torch.Size([3, 64, 568, 568])\n",
      "before max pool torch.Size([3, 128, 280, 280])\n",
      "before max pool torch.Size([3, 256, 136, 136])\n",
      "before max pool torch.Size([3, 512, 64, 64])\n",
      "convolved bottleneck:  torch.Size([3, 1024, 28, 28])\n",
      "0\n",
      "ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 512, 56, 56]) torch.Size([3, 512, 64, 64])\n",
      "doubleconv id 1\n",
      "2\n",
      "ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 256, 104, 104]) torch.Size([3, 256, 136, 136])\n",
      "doubleconv id 3\n",
      "4\n",
      "ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 128, 200, 200]) torch.Size([3, 128, 280, 280])\n",
      "doubleconv id 5\n",
      "6\n",
      "ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 64, 392, 392]) torch.Size([3, 64, 568, 568])\n",
      "doubleconv id 7\n"
     ]
    }
   ],
   "source": [
    "#model gets as input a single sample -sample image converted to torch tensor- outputs another image \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = torch.rand((3,3,572,572))\n",
    "    model = UNet()\n",
    "    print(model(image))\n",
    "    output = model(image)\n",
    "    model.forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0017e30",
   "metadata": {},
   "source": [
    "## Data Set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a775222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from imageio import imread\n",
    "import os\n",
    "\n",
    "class DT_DataSet(Dataset):\n",
    "    #when an object of class DT_DataSet is created the attributes given would be masks and images folder names\n",
    "    def __init__(self,masks_path,images_path,transform = None):\n",
    "        self.masks_path = masks_path\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        #load the whole dataset, holds name of each single image in the dataset\n",
    "        self.masks = sorted(os.listdir(self.masks_path))\n",
    "        self.images = sorted(os.listdir(self.images_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.masks)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #loads single data sample in the dataset, dataloader uses __getitem__ to get a single pair of (image,mask)\n",
    "        #combines multiple of these together to form a batch\n",
    "    \n",
    "        #combine image name with the folder name to create a path for the image(onesingle image in the dataset not the whole set)\n",
    "        mask_filename,image_filename = os.path.join(self.masks_path, self.masks[index]),os.path.join(self.images_path, self.images[index])\n",
    "        \n",
    "        #read the image as an array\n",
    "        mask, image = imread(mask_filename), imread(image_filename)\n",
    "     \n",
    "        if self.transform != None:\n",
    "            mask, image = self.transform(mask),self.transform(image)\n",
    "            \n",
    "        return mask, image\n",
    "#give folder name iterate through each file in the folder\n",
    "#dataset object holds the entire dataset\n",
    "#train data will be augmented images, validation data will be training images\n",
    "val_mask_path = r\"C:\\Users\\derbent.z\\Desktop\\Training attempts\\training_0302\\mask train set\"\n",
    "val_image_path = r\"C:\\Users\\derbent.z\\Desktop\\Training attempts\\training_0302\\train set\"\n",
    "dataset = DT_DataSet(val_mask_path, val_image_path,transforms.ToTensor())\n",
    "#dataloader loads this dataset in batched for the training of the network\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
