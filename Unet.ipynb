{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee52967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629b6d0d",
   "metadata": {},
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "            #def conv to be applied\n",
    "\n",
    "    \n",
    "    conv = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(in_channels, out_channels, 3 ,1 ,1 , bias = False), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace = True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, 3 ,1 ,1 , bias = False),\n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace = True)\n",
    "            \n",
    "                            )\n",
    "\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdbf531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThink about the sizes of the feature maps. 500x500 downsamples to 250x250, okay. \\nThat downsamples to 125x125, okay. But what do you get when you downsample 125x125 \\nby factor 2 (as the max pooling layers do)? Can you have feature maps with fractions\\nof sizes (no)? Accordingly, this will result in feature maps of size 62x62 (basically \\ncutting off one pixel in the input), which will later upsample back to 124x124 and create\\na mismatch.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Think about the sizes of the feature maps. 500x500 downsamples to 250x250, okay. \n",
    "That downsamples to 125x125, okay. But what do you get when you downsample 125x125 \n",
    "by factor 2 (as the max pooling layers do)? Can you have feature maps with fractions\n",
    "of sizes (no)? Accordingly, this will result in feature maps of size 62x62 (basically \n",
    "cutting off one pixel in the input), which will later upsample back to 124x124 and create\n",
    "a mismatch.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f22755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_sc(tensor, target_tensor): #crop the skip connection to size of upsample target tensor-> upsample, tensor-> skip connection\n",
    "    #target_size = target_tensor.size()[2] #batch size, channels, height, width\n",
    "    #tensor_size = tensor.size()[2]\n",
    "    #delta = (tensor_size - target_size)//2 #64 - 56\n",
    "    \n",
    "    tensor = TF.resize(tensor, size=target_tensor.shape[2:]) \n",
    "    #print('tensor_size ',tensor_size,'target_size; ',target_size,'delta:',delta)\n",
    "\n",
    "    return tensor\n",
    "    #tensor[:, :, delta:tensor_size - delta, delta:tensor_size - delta] #4:60 , 4:60\n",
    "\n",
    "def double_conv(in_ch, out_ch):\n",
    "        #def conv to be applied\n",
    "    conv = nn.Sequential( #apply 2 conv sequantially\n",
    "        nn.Conv2d(in_ch, out_ch, 3),\n",
    "        nn.ReLU(inplace = True), \n",
    "            \n",
    "        nn.Conv2d(out_ch, out_ch, 3),\n",
    "        nn.ReLU(inplace = True)\n",
    "    )\n",
    "    return conv\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 3, features = [64,128,256,512,1024]):\n",
    "        \n",
    "        super(UNet, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        #bunlar module ile liste icinde tutulabilir \n",
    "        \n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        \n",
    "        for feature in features:\n",
    "            #self.d_conv = double_conv(in_channels,feature)\n",
    "            print(in_channels,feature)\n",
    "            self.downs.append(double_conv(in_channels,feature))\n",
    "            in_channels = feature\n",
    "            \n",
    "            \n",
    "        for feature in reversed(features[1:]):\n",
    "\n",
    "            print('transpose: ', feature, feature//2)\n",
    "            self.ups.append(nn.ConvTranspose2d(feature,feature//2,2,2))\n",
    "            self.ups.append(double_conv(feature,feature//2))\n",
    "            \n",
    "        print('bottleneck', features[-1]//2,features[-1])\n",
    "        \n",
    "        self.bottleneck = double_conv(features[-1]//2,features[-1]) #512, 1024\n",
    "        self.out = nn.Conv2d(features[0],2,3) #64,2,1\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []  \n",
    "        print(\"x\",x.size())\n",
    "        for down in self.downs[:-1]:\n",
    "            x = down(x)\n",
    "            print(\"before max pool\",x.size())\n",
    "            skip_connections.append(x) #we will use this in concatination with up parts\n",
    "            x = self.max_pool(x)\n",
    "            \n",
    "            \n",
    "        x = self.bottleneck(x) #apply the doubleconv at the bottleneck step, bottleneck ile basla\n",
    "        print(\"convolved bottleneck: \",x.size())\n",
    "        \n",
    "        skip_connections = skip_connections[::-1] #to reverse the list\n",
    "        \n",
    "        for idx in range(0, len(self.ups) , 2):\n",
    "            print(idx)\n",
    "            print(self.ups[idx])\n",
    "            x = self.ups[idx](x) # apply transpose conv, start with bottleneck \n",
    "            s_con = skip_connections[idx//2] #bc idx = 0,2,4,6,8.. get skip connection, skip connection is the last layer form contracting path\n",
    "            print(\"sizes\", x.size(),s_con.size())\n",
    "            #s_con is 64x64x512, 136x136x256, \n",
    "            y = crop_sc(s_con,x) #crop the skip connectio 64 needs to be resized to 56\n",
    "            \n",
    "                #concat\n",
    "            concat = torch.cat([x,y] ,1) #concat bottleneck 512x56x56 with cropped 64 from contracting layer (56x56x512)\n",
    "            ##element wise addition output is 56x56x512\n",
    "            print('doubleconv id',idx+1)\n",
    "            x = self.ups[idx+1](concat) #apply double conv\n",
    "            #double convs are stored in indexes 1,3,5\n",
    "            print('first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko', x.size())\n",
    "        print('hamd')                    \n",
    "        return self.out(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a04aeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "#transform = T.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "293ef0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 64\n",
      "64 128\n",
      "128 256\n",
      "256 512\n",
      "512 1024\n",
      "transpose:  1024 512\n",
      "transpose:  512 256\n",
      "transpose:  256 128\n",
      "transpose:  128 64\n",
      "bottleneck 512 1024\n",
      "x torch.Size([3, 3, 572, 572])\n",
      "before max pool torch.Size([3, 64, 568, 568])\n",
      "before max pool torch.Size([3, 128, 280, 280])\n",
      "before max pool torch.Size([3, 256, 136, 136])\n",
      "before max pool torch.Size([3, 512, 64, 64])\n",
      "convolved bottleneck:  torch.Size([3, 1024, 28, 28])\n",
      "0\n",
      "ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 512, 56, 56]) torch.Size([3, 512, 64, 64])\n",
      "doubleconv id 1\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 512, 52, 52])\n",
      "2\n",
      "ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 256, 104, 104]) torch.Size([3, 256, 136, 136])\n",
      "doubleconv id 3\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 256, 100, 100])\n",
      "4\n",
      "ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 128, 200, 200]) torch.Size([3, 128, 280, 280])\n",
      "doubleconv id 5\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 128, 196, 196])\n",
      "6\n",
      "ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 64, 392, 392]) torch.Size([3, 64, 568, 568])\n",
      "doubleconv id 7\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 64, 388, 388])\n",
      "hamd\n",
      "tensor([[[[-2.4609e-03,  9.0620e-05, -5.1906e-03,  ..., -4.9956e-03,\n",
      "           -4.6248e-03, -3.5401e-03],\n",
      "          [-6.0603e-03, -5.6083e-03, -1.7172e-03,  ..., -4.8918e-03,\n",
      "           -6.0429e-03, -6.6695e-03],\n",
      "          [-3.0668e-03, -4.4249e-03,  4.0636e-04,  ..., -1.6099e-03,\n",
      "           -4.6897e-03, -8.3911e-03],\n",
      "          ...,\n",
      "          [-4.0555e-03, -4.5279e-03, -2.7797e-03,  ..., -2.5847e-03,\n",
      "           -1.4549e-03, -5.7685e-03],\n",
      "          [-2.6978e-03, -2.1972e-03, -1.2230e-03,  ..., -1.7518e-03,\n",
      "           -2.1697e-03, -4.1902e-03],\n",
      "          [-2.2289e-03, -5.9744e-03, -5.2612e-03,  ..., -4.5189e-03,\n",
      "           -1.6997e-03, -2.5423e-03]],\n",
      "\n",
      "         [[ 8.5152e-03,  1.1984e-02,  1.3301e-02,  ...,  1.0356e-02,\n",
      "            9.9286e-03,  8.9646e-03],\n",
      "          [ 9.9698e-03,  9.3918e-03,  1.0384e-02,  ...,  1.2486e-02,\n",
      "            7.6815e-03,  1.1245e-02],\n",
      "          [ 9.1441e-03,  1.2125e-02,  9.0660e-03,  ...,  8.9675e-03,\n",
      "            1.0190e-02,  1.1036e-02],\n",
      "          ...,\n",
      "          [ 9.1026e-03,  1.4169e-02,  1.1493e-02,  ...,  9.0978e-03,\n",
      "            8.9944e-03,  1.1529e-02],\n",
      "          [ 1.1391e-02,  1.1551e-02,  9.5115e-03,  ...,  1.1424e-02,\n",
      "            9.9469e-03,  1.3465e-02],\n",
      "          [ 1.1142e-02,  1.2036e-02,  6.7862e-03,  ...,  1.0699e-02,\n",
      "            1.2192e-02,  1.2672e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.1722e-03, -1.9197e-03, -1.3929e-04,  ..., -4.5041e-03,\n",
      "           -3.2210e-03, -3.2521e-04],\n",
      "          [-6.6325e-03, -5.8698e-03, -5.5334e-03,  ..., -5.4335e-03,\n",
      "           -5.1988e-03, -6.4416e-03],\n",
      "          [-2.0524e-03, -2.2246e-03, -4.3228e-03,  ..., -6.4717e-04,\n",
      "           -2.1413e-03, -5.7637e-03],\n",
      "          ...,\n",
      "          [-5.0206e-03, -5.5303e-03, -4.5179e-03,  ..., -4.3142e-03,\n",
      "           -4.6637e-03, -5.3784e-03],\n",
      "          [-1.5528e-03, -4.5438e-03, -2.0525e-03,  ..., -7.3396e-03,\n",
      "           -3.3042e-03, -5.3997e-03],\n",
      "          [-3.9053e-03, -1.5572e-03, -1.9569e-03,  ..., -1.8594e-03,\n",
      "           -3.3239e-03, -3.0209e-03]],\n",
      "\n",
      "         [[ 1.2810e-02,  1.2140e-02,  1.0344e-02,  ...,  1.1098e-02,\n",
      "            1.0507e-02,  1.3880e-02],\n",
      "          [ 1.0425e-02,  1.2090e-02,  8.8742e-03,  ...,  1.3217e-02,\n",
      "            9.3732e-03,  1.0912e-02],\n",
      "          [ 1.1541e-02,  1.1152e-02,  1.0299e-02,  ...,  8.1362e-03,\n",
      "            1.0784e-02,  9.8481e-03],\n",
      "          ...,\n",
      "          [ 1.0186e-02,  1.0467e-02,  1.0366e-02,  ...,  1.2120e-02,\n",
      "            1.2019e-02,  1.0355e-02],\n",
      "          [ 1.3699e-02,  1.3519e-02,  9.7896e-03,  ...,  9.0135e-03,\n",
      "            1.0283e-02,  1.0876e-02],\n",
      "          [ 9.8986e-03,  1.0135e-02,  1.2046e-02,  ...,  8.6766e-03,\n",
      "            1.2056e-02,  1.2761e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.9857e-06, -4.2051e-03, -3.6658e-03,  ..., -3.3025e-03,\n",
      "           -6.4930e-03, -2.9483e-03],\n",
      "          [-2.0003e-03, -4.3135e-03, -7.8717e-04,  ..., -2.6594e-03,\n",
      "           -2.6312e-03, -3.5512e-03],\n",
      "          [-3.2409e-03, -3.3137e-03, -4.3455e-03,  ..., -2.6496e-03,\n",
      "           -1.5007e-03, -1.1959e-03],\n",
      "          ...,\n",
      "          [-2.2054e-03, -2.1481e-03, -1.0041e-04,  ..., -4.9641e-03,\n",
      "           -2.8993e-03, -5.0906e-03],\n",
      "          [-4.1265e-03, -7.7592e-03, -3.9560e-03,  ..., -1.9249e-03,\n",
      "           -3.9981e-03, -6.4087e-04],\n",
      "          [-4.0666e-03, -3.6849e-03, -2.9224e-03,  ..., -3.7955e-03,\n",
      "           -2.1866e-03, -3.7842e-03]],\n",
      "\n",
      "         [[ 9.0284e-03,  1.0868e-02,  7.9687e-03,  ...,  1.0339e-02,\n",
      "            1.0557e-02,  1.2919e-02],\n",
      "          [ 7.9693e-03,  1.1993e-02,  1.2279e-02,  ...,  1.0620e-02,\n",
      "            8.7551e-03,  1.4484e-02],\n",
      "          [ 9.6476e-03,  6.5211e-03,  8.6179e-03,  ...,  8.4541e-03,\n",
      "            8.2103e-03,  9.0479e-03],\n",
      "          ...,\n",
      "          [ 1.1989e-02,  1.2065e-02,  7.9811e-03,  ...,  1.0837e-02,\n",
      "            1.0084e-02,  8.5203e-03],\n",
      "          [ 1.0583e-02,  6.0161e-03,  9.4345e-03,  ...,  1.3860e-02,\n",
      "            6.1101e-03,  1.3539e-02],\n",
      "          [ 1.1414e-02,  1.2192e-02,  1.4241e-02,  ...,  8.9563e-03,\n",
      "            1.1619e-02,  1.0716e-02]]]], grad_fn=<ConvolutionBackward0>)\n",
      "x torch.Size([3, 3, 572, 572])\n",
      "before max pool torch.Size([3, 64, 568, 568])\n",
      "before max pool torch.Size([3, 128, 280, 280])\n",
      "before max pool torch.Size([3, 256, 136, 136])\n",
      "before max pool torch.Size([3, 512, 64, 64])\n",
      "convolved bottleneck:  torch.Size([3, 1024, 28, 28])\n",
      "0\n",
      "ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 512, 56, 56]) torch.Size([3, 512, 64, 64])\n",
      "doubleconv id 1\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 512, 52, 52])\n",
      "2\n",
      "ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 256, 104, 104]) torch.Size([3, 256, 136, 136])\n",
      "doubleconv id 3\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 256, 100, 100])\n",
      "4\n",
      "ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 128, 200, 200]) torch.Size([3, 128, 280, 280])\n",
      "doubleconv id 5\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 128, 196, 196])\n",
      "6\n",
      "ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "sizes torch.Size([3, 64, 392, 392]) torch.Size([3, 64, 568, 568])\n",
      "doubleconv id 7\n",
      "first layer done out should be 56x56x1024, now apply double conv output should be 52x52x512 cunku topluyoruy birbirne eklemiyoruy asko torch.Size([3, 64, 388, 388])\n",
      "hamd\n"
     ]
    }
   ],
   "source": [
    "#model gets as input a single sample -sample image converted to torch tensor- outputs another image \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image = torch.rand((3,3,572,572))\n",
    "    model = UNet()\n",
    "    print(model(image))\n",
    "    output = model(image)\n",
    "    model.forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee105730",
   "metadata": {},
   "source": [
    "    '''\n",
    "    image = Image.open(r\"C:\\Users\\derbent.z\\Desktop\\images\\All Images\\new_data\\img_0202\\20220222_101933._0.bmp\")\n",
    "    image.show()\n",
    "    convert_tensor = transforms.ToTensor()\n",
    "    image = convert_tensor(image)\n",
    "    image = image.unsqueeze(0)\n",
    "    print(image)\n",
    "    print(image.shape)\n",
    "    '''\n",
    "        #transform = T.ToPILImage()\n",
    "    #img = transform(output)\n",
    "    '''im = output[0]\n",
    "    save_image(im, \"img1.png\")\n",
    "    '''\n",
    "    #im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0017e30",
   "metadata": {},
   "source": [
    "## Data Set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a775222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from imageio import imread\n",
    "import os\n",
    "\n",
    "class DT_DataSet(Dataset):\n",
    "    #when an object of class DT_DataSet is created the attributes given would be masks and images folder names\n",
    "    def __init__(self,masks_path,images_path,transform = None):\n",
    "        self.masks_path = masks_path\n",
    "        self.images_path = images_path\n",
    "        self.transform = transform\n",
    "        \n",
    "        #load the whole dataset, holds name of each single image in the dataset\n",
    "        self.masks = sorted(os.listdir(self.masks_path))\n",
    "        self.images = sorted(os.listdir(self.images_path))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.masks)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        #loads single data sample in the dataset, dataloader uses __getitem__ to get a single pair of (image,mask)\n",
    "        #combines multiple of these together to form a batch\n",
    "    \n",
    "        #combine image name with the folder name to create a path for the image(onesingle image in the dataset not the whole set)\n",
    "        mask_filename,image_filename = os.path.join(self.masks_path, self.masks[index]),os.path.join(self.images_path, self.images[index])\n",
    "        \n",
    "        #read the image as an array\n",
    "        mask, image = imread(mask_filename), imread(image_filename)\n",
    "     \n",
    "        if self.transform != None:\n",
    "            mask, image = self.transform(mask),self.transform(image)\n",
    "            \n",
    "        return mask, image\n",
    "#give folder name iterate through each file in the folder\n",
    "#dataset object holds the entire dataset\n",
    "#train data will be augmented images, validation data will be training images\n",
    "val_mask_path = r\"C:\\Users\\derbent.z\\Desktop\\Training attempts\\training_0302\\mask train set\"\n",
    "val_image_path = r\"C:\\Users\\derbent.z\\Desktop\\Training attempts\\training_0302\\train set\"\n",
    "dataset = DT_DataSet(val_mask_path, val_image_path,transforms.ToTensor())\n",
    "#dataloader loads this dataset in batched for the training of the network\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87f817",
   "metadata": {},
   "source": [
    "run training -> epoch size kadar bir iteration olacak, main metod veya run trianingde yapilabilir \n",
    "\n",
    "her epoch icin batch size kadar bir iteration olacak  \n",
    "her batch sizeda tarinset(augmented images) pairler dönülecek\n",
    "\n",
    "loss epoch sonunda hesaplanacak? yoksa her batchtan sonra mi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e457ff8",
   "metadata": {},
   "source": [
    "## Train the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d078ba",
   "metadata": {},
   "source": [
    "To train the network:\n",
    "- **batch size:** 32\n",
    "- **learning rate:** 0.001\n",
    "- **epoch size:** 100\n",
    "- **optimizer:** Adam\n",
    "- **loss function:** Cross entropy\n",
    "- **val set:** train images\n",
    "- **train set:** augmented set\n",
    "\n",
    "- model: e.g. the U-Net\n",
    "- device: CPU or GPU\n",
    "- criterion: loss function (e.g. CrossEntropyLoss, DiceCoefficientLoss)\n",
    "- optimizer: e.g. SGD\n",
    "- training_DataLoader: a training dataloader\n",
    "- validation_DataLoader: a validation dataloader\n",
    "- lr_scheduler: a learning rate scheduler (optional)\n",
    "- epochs: The number of epochs we want to train\n",
    "- epoch: The epoch number from where training should start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5614ae1",
   "metadata": {},
   "source": [
    "1- In train function iterate over the dataloader (holds the whole dataset and splits into batches) \n",
    "2- train the model\n",
    "3- calculate loss by comparin pred outputmask with gt output mask \n",
    "4- store the loss's in a list\n",
    "5- apply backward pass\n",
    "6- use gradient\n",
    "7- use optimizer to update the model parameters\n",
    "7- compare loss's \n",
    "8- determine the lowest loss \n",
    "9- training over\n",
    "\n",
    "In validation:\n",
    "- iterate over val set\n",
    "- calc loss\n",
    "- determine the best loss\n",
    "- no gradient no backprop, we are not learning in validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f6947",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training the model:\n",
    "    - Determine the epoch size\n",
    "    - split training set to batches (of 32)\n",
    "    - for each sample in the batch in a single epoch (50 batches in a single epoch 32 samples in a single batch):\n",
    "        run forward pass\n",
    "        calc loss\n",
    "        run backprop\n",
    "        compute gradients respective to the loss func.\n",
    "        add to accumulated gradients\n",
    "    - Obtain accumulated gradients at the end of each batch \n",
    "    - adjust the parameters at the end of each batch by using the Adam optimizer\n",
    "    - after each batch in a single epoch is complete\n",
    "    - run validation set through the network store the loss (after every epoch)\n",
    "    - Run the remaining batches, if the loss is not decreasing then save the model parameters with the lowest loss\n",
    "    \n",
    "    -the input image is going to be a 4d torch tensor input is 3 channels output is 3 channels (later when adding the splice it will be 4 channels)\n",
    "    - convert the 3d tensor to 4d tensor to use in training the model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbbc0b7",
   "metadata": {},
   "source": [
    "## Training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a40e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#epoch size 100\n",
    "#dataloader splits the dataset into size 32 batches\n",
    "class Train():\n",
    "    def __init__(self,model,criterion,backprop,optimizer,train_data, val_data,epoch,batch_size):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.backprop = backprop\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data\n",
    "        self.val_data\n",
    "        \n",
    "\n",
    "def train():\n",
    "    #model() my model gets as input a single image as a torch tensor\n",
    "    for image, mask, gt in batch:\n",
    "        out = self.model(image,mask,gt)\n",
    "        loss = self.criterion(out, gt)\n",
    "        self.backprop(loss)\n",
    "        self.optimizer()\n",
    "        \n",
    "        \n",
    "def run_train():\n",
    "    \n",
    "    for i in range(epoch_size):\n",
    "        train()\n",
    "        run on validation \n",
    "        val_loss.append(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de01d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def autocrop(encoder_layer: torch.Tensor, decoder_layer: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Center-crops the encoder_layer to the size of the decoder_layer,\n",
    "    so that merging (concatenation) between levels/blocks is possible.\n",
    "    This is only necessary for input sizes != 2**n for 'same' padding and always required for 'valid' padding.\n",
    "    \"\"\"\n",
    "    if encoder_layer.shape[2:] != decoder_layer.shape[2:]:\n",
    "        ds = encoder_layer.shape[2:]\n",
    "        es = decoder_layer.shape[2:]\n",
    "        assert ds[0] >= es[0]\n",
    "        assert ds[1] >= es[1]\n",
    "        if encoder_layer.dim() == 4:  # 2D\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2)\n",
    "                            ]\n",
    "        elif encoder_layer.dim() == 5:  # 3D\n",
    "            assert ds[2] >= es[2]\n",
    "            encoder_layer = encoder_layer[\n",
    "                            :,\n",
    "                            :,\n",
    "                            ((ds[0] - es[0]) // 2):((ds[0] + es[0]) // 2),\n",
    "                            ((ds[1] - es[1]) // 2):((ds[1] + es[1]) // 2),\n",
    "                            ((ds[2] - es[2]) // 2):((ds[2] + es[2]) // 2),\n",
    "                            ]\n",
    "    return encoder_layer, decoder_layer\n",
    "\n",
    "\n",
    "def conv_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.Conv3d\n",
    "    elif dim == 2:\n",
    "        return nn.Conv2d\n",
    "\n",
    "\n",
    "def get_conv_layer(in_channels: int,\n",
    "                   out_channels: int,\n",
    "                   kernel_size: int = 3,\n",
    "                   stride: int = 1,\n",
    "                   padding: int = 1,\n",
    "                   bias: bool = True,\n",
    "                   dim: int = 2):\n",
    "    return conv_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                           bias=bias)\n",
    "\n",
    "\n",
    "def conv_transpose_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.ConvTranspose3d\n",
    "    elif dim == 2:\n",
    "        return nn.ConvTranspose2d\n",
    "\n",
    "\n",
    "def get_up_layer(in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 2,\n",
    "                 stride: int = 2,\n",
    "                 dim: int = 3,\n",
    "                 up_mode: str = 'transposed',\n",
    "                 ):\n",
    "    if up_mode == 'transposed':\n",
    "        return conv_transpose_layer(dim)(in_channels, out_channels, kernel_size=kernel_size, stride=stride)\n",
    "    else:\n",
    "        return nn.Upsample(scale_factor=2.0, mode=up_mode)\n",
    "\n",
    "\n",
    "def maxpool_layer(dim: int):\n",
    "    if dim == 3:\n",
    "        return nn.MaxPool3d\n",
    "    elif dim == 2:\n",
    "        return nn.MaxPool2d\n",
    "\n",
    "\n",
    "def get_maxpool_layer(kernel_size: int = 2,\n",
    "                      stride: int = 2,\n",
    "                      padding: int = 0,\n",
    "                      dim: int = 2):\n",
    "    return maxpool_layer(dim=dim)(kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "\n",
    "def get_activation(activation: str):\n",
    "    if activation == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif activation == 'leaky':\n",
    "        return nn.LeakyReLU(negative_slope=0.1)\n",
    "    elif activation == 'elu':\n",
    "        return nn.ELU()\n",
    "\n",
    "\n",
    "def get_normalization(normalization: str,\n",
    "                      num_channels: int,\n",
    "                      dim: int):\n",
    "    if normalization == 'batch':\n",
    "        if dim == 3:\n",
    "            return nn.BatchNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.BatchNorm2d(num_channels)\n",
    "    elif normalization == 'instance':\n",
    "        if dim == 3:\n",
    "            return nn.InstanceNorm3d(num_channels)\n",
    "        elif dim == 2:\n",
    "            return nn.InstanceNorm2d(num_channels)\n",
    "    elif 'group' in normalization:\n",
    "        num_groups = int(normalization.partition('group')[-1])  # get the group size from string\n",
    "        return nn.GroupNorm(num_groups=num_groups, num_channels=num_channels)\n",
    "\n",
    "\n",
    "class Concatenate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Concatenate, self).__init__()\n",
    "\n",
    "    def forward(self, layer_1, layer_2):\n",
    "        x = torch.cat((layer_1, layer_2), 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 MaxPool.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 pooling: bool = True,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: str = 2,\n",
    "                 conv_mode: str = 'same'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.pooling = pooling\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "\n",
    "        # conv layers\n",
    "        self.conv1 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # pooling layer\n",
    "        if self.pooling:\n",
    "            self.pool = get_maxpool_layer(kernel_size=2, stride=2, padding=0, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # activation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "\n",
    "        before_pooling = y  # save the outputs before the pooling operation\n",
    "        if self.pooling:\n",
    "            y = self.pool(y)  # pooling\n",
    "        return y, before_pooling\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A helper Module that performs 2 Convolutions and 1 UpConvolution/Upsample.\n",
    "    An activation follows each convolution.\n",
    "    A normalization layer follows each convolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = None,\n",
    "                 dim: int = 3,\n",
    "                 conv_mode: str = 'same',\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalization = normalization\n",
    "        if conv_mode == 'same':\n",
    "            self.padding = 1\n",
    "        elif conv_mode == 'valid':\n",
    "            self.padding = 0\n",
    "        self.dim = dim\n",
    "        self.activation = activation\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        # upconvolution/upsample layer\n",
    "        self.up = get_up_layer(self.in_channels, self.out_channels, kernel_size=2, stride=2, dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "        # conv layers\n",
    "        self.conv0 = get_conv_layer(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv1 = get_conv_layer(2 * self.out_channels, self.out_channels, kernel_size=3, stride=1,\n",
    "                                    padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "        self.conv2 = get_conv_layer(self.out_channels, self.out_channels, kernel_size=3, stride=1, padding=self.padding,\n",
    "                                    bias=True, dim=self.dim)\n",
    "\n",
    "        # activation layers\n",
    "        self.act0 = get_activation(self.activation)\n",
    "        self.act1 = get_activation(self.activation)\n",
    "        self.act2 = get_activation(self.activation)\n",
    "\n",
    "        # normalization layers\n",
    "        if self.normalization:\n",
    "            self.norm0 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm1 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "            self.norm2 = get_normalization(normalization=self.normalization, num_channels=self.out_channels,\n",
    "                                           dim=self.dim)\n",
    "\n",
    "        # concatenate layer\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def forward(self, encoder_layer, decoder_layer):\n",
    "        \"\"\" Forward pass\n",
    "        Arguments:\n",
    "            encoder_layer: Tensor from the encoder pathway\n",
    "            decoder_layer: Tensor from the decoder pathway (to be up'd)\n",
    "        \"\"\"\n",
    "        up_layer = self.up(decoder_layer)  # up-convolution/up-sampling\n",
    "        cropped_encoder_layer, dec_layer = autocrop(encoder_layer, up_layer)  # cropping\n",
    "\n",
    "        if self.up_mode != 'transposed':\n",
    "            # We need to reduce the channel dimension with a conv layer\n",
    "            up_layer = self.conv0(up_layer)  # convolution 0\n",
    "        up_layer = self.act0(up_layer)  # activation 0\n",
    "        if self.normalization:\n",
    "            up_layer = self.norm0(up_layer)  # normalization 0\n",
    "\n",
    "        merged_layer = self.concat(up_layer, cropped_encoder_layer)  # concatenation\n",
    "        y = self.conv1(merged_layer)  # convolution 1\n",
    "        y = self.act1(y)  # activation 1\n",
    "        if self.normalization:\n",
    "            y = self.norm1(y)  # normalization 1\n",
    "        y = self.conv2(y)  # convolution 2\n",
    "        y = self.act2(y)  # acivation 2\n",
    "        if self.normalization:\n",
    "            y = self.norm2(y)  # normalization 2\n",
    "        return y\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 out_channels: int = 2,\n",
    "                 n_blocks: int = 4,\n",
    "                 start_filters: int = 32,\n",
    "                 activation: str = 'relu',\n",
    "                 normalization: str = 'batch',\n",
    "                 conv_mode: str = 'same',\n",
    "                 dim: int = 2,\n",
    "                 up_mode: str = 'transposed'\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.n_blocks = n_blocks\n",
    "        self.start_filters = start_filters\n",
    "        self.activation = activation\n",
    "        self.normalization = normalization\n",
    "        self.conv_mode = conv_mode\n",
    "        self.dim = dim\n",
    "        self.up_mode = up_mode\n",
    "\n",
    "        self.down_blocks = []\n",
    "        self.up_blocks = []\n",
    "\n",
    "        # create encoder path\n",
    "        for i in range(self.n_blocks):\n",
    "            num_filters_in = self.in_channels if i == 0 else num_filters_out\n",
    "            num_filters_out = self.start_filters * (2 ** i)\n",
    "            pooling = True if i < self.n_blocks - 1 else False\n",
    "\n",
    "            down_block = DownBlock(in_channels=num_filters_in,\n",
    "                                   out_channels=num_filters_out,\n",
    "                                   pooling=pooling,\n",
    "                                   activation=self.activation,\n",
    "                                   normalization=self.normalization,\n",
    "                                   conv_mode=self.conv_mode,\n",
    "                                   dim=self.dim)\n",
    "\n",
    "            self.down_blocks.append(down_block)\n",
    "\n",
    "        # create decoder path (requires only n_blocks-1 blocks)\n",
    "        for i in range(n_blocks - 1):\n",
    "            num_filters_in = num_filters_out\n",
    "            num_filters_out = num_filters_in // 2\n",
    "\n",
    "            up_block = UpBlock(in_channels=num_filters_in,\n",
    "                               out_channels=num_filters_out,\n",
    "                               activation=self.activation,\n",
    "                               normalization=self.normalization,\n",
    "                               conv_mode=self.conv_mode,\n",
    "                               dim=self.dim,\n",
    "                               up_mode=self.up_mode)\n",
    "\n",
    "            self.up_blocks.append(up_block)\n",
    "\n",
    "        # final convolution\n",
    "        self.conv_final = get_conv_layer(num_filters_out, self.out_channels, kernel_size=1, stride=1, padding=0,\n",
    "                                         bias=True, dim=self.dim)\n",
    "\n",
    "        # add the list of modules to current module\n",
    "        self.down_blocks = nn.ModuleList(self.down_blocks)\n",
    "        self.up_blocks = nn.ModuleList(self.up_blocks)\n",
    "\n",
    "        # initialize the weights\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    @staticmethod\n",
    "    def weight_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.weight, **kwargs)  # weights\n",
    "\n",
    "    @staticmethod\n",
    "    def bias_init(module, method, **kwargs):\n",
    "        if isinstance(module, (nn.Conv3d, nn.Conv2d, nn.ConvTranspose3d, nn.ConvTranspose2d)):\n",
    "            method(module.bias, **kwargs)  # bias\n",
    "\n",
    "    def initialize_parameters(self,\n",
    "                              method_weights=nn.init.xavier_uniform_,\n",
    "                              method_bias=nn.init.zeros_,\n",
    "                              kwargs_weights={},\n",
    "                              kwargs_bias={}\n",
    "                              ):\n",
    "        for module in self.modules():\n",
    "            self.weight_init(module, method_weights, **kwargs_weights)  # initialize weights\n",
    "            self.bias_init(module, method_bias, **kwargs_bias)  # initialize bias\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        encoder_output = []\n",
    "\n",
    "        # Encoder pathway\n",
    "        for module in self.down_blocks:\n",
    "            x, before_pooling = module(x)\n",
    "            encoder_output.append(before_pooling)\n",
    "\n",
    "        # Decoder pathway\n",
    "        for i, module in enumerate(self.up_blocks):\n",
    "            before_pool = encoder_output[-(i + 2)]\n",
    "            x = module(before_pool, x)\n",
    "\n",
    "        x = self.conv_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __repr__(self):\n",
    "        attributes = {attr_key: self.__dict__[attr_key] for attr_key in self.__dict__.keys() if '_' not in attr_key[0] and 'training' not in attr_key}\n",
    "        d = {self.__class__.__name__: attributes}\n",
    "        return f'{d}'\n",
    "view rawunet.py hosted with ❤ by GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd33e650",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3262890820.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\derbent.z\\AppData\\Local\\Temp\\ipykernel_15628\\3262890820.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    sample training:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sample training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027c2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: torch.nn.Module,\n",
    "                 device: torch.device,\n",
    "                 criterion: torch.nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 training_DataLoader: torch.utils.data.Dataset,\n",
    "                 validation_DataLoader: torch.utils.data.Dataset = None,\n",
    "                 lr_scheduler: torch.optim.lr_scheduler = None,\n",
    "                 epochs: int = 100,\n",
    "                 epoch: int = 0,\n",
    "                 notebook: bool = False\n",
    "                 ):\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.training_DataLoader = training_DataLoader\n",
    "        self.validation_DataLoader = validation_DataLoader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.epoch = epoch\n",
    "        self.notebook = notebook\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.learning_rate = []\n",
    "\n",
    "    def run_trainer(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        progressbar = trange(self.epochs, desc='Progress')\n",
    "        for i in progressbar:\n",
    "            \"\"\"Epoch counter\"\"\"\n",
    "            self.epoch += 1  # epoch counter\n",
    "\n",
    "            \"\"\"Training block\"\"\"\n",
    "            self._train()\n",
    "\n",
    "            \"\"\"Validation block\"\"\"\n",
    "            if self.validation_DataLoader is not None:\n",
    "                self._validate()\n",
    "\n",
    "            \"\"\"Learning rate scheduler block\"\"\"\n",
    "            if self.lr_scheduler is not None:\n",
    "                if self.validation_DataLoader is not None and self.lr_scheduler.__class__.__name__ == 'ReduceLROnPlateau':\n",
    "                    self.lr_scheduler.batch(self.validation_loss[i])  # learning rate scheduler step with validation loss\n",
    "                else:\n",
    "                    self.lr_scheduler.batch()  # learning rate scheduler step\n",
    "        return self.training_loss, self.validation_loss, self.learning_rate\n",
    "\n",
    "    def _train(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.train()  # train mode\n",
    "        train_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.training_DataLoader), 'Training', total=len(self.training_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "            self.optimizer.zero_grad()  # zerograd the parameters\n",
    "            out = self.model(input)  # one forward pass\n",
    "            loss = self.criterion(out, target)  # calculate loss\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            loss.backward()  # one backward pass\n",
    "            self.optimizer.step()  # update the parameters\n",
    "\n",
    "            batch_iter.set_description(f'Training: (loss {loss_value:.4f})')  # update progressbar\n",
    "\n",
    "        self.training_loss.append(np.mean(train_losses))\n",
    "        self.learning_rate.append(self.optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        batch_iter.close()\n",
    "\n",
    "    def _validate(self):\n",
    "\n",
    "        if self.notebook:\n",
    "            from tqdm.notebook import tqdm, trange\n",
    "        else:\n",
    "            from tqdm import tqdm, trange\n",
    "\n",
    "        self.model.eval()  # evaluation mode\n",
    "        valid_losses = []  # accumulate the losses here\n",
    "        batch_iter = tqdm(enumerate(self.validation_DataLoader), 'Validation', total=len(self.validation_DataLoader),\n",
    "                          leave=False)\n",
    "\n",
    "        for i, (x, y) in batch_iter:\n",
    "            input, target = x.to(self.device), y.to(self.device)  # send to device (GPU or CPU)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out = self.model(input)\n",
    "                loss = self.criterion(out, target)\n",
    "                loss_value = loss.item()\n",
    "                valid_losses.append(loss_value)\n",
    "\n",
    "                batch_iter.set_description(f'Validation: (loss {loss_value:.4f})')\n",
    "\n",
    "        self.validation_loss.append(np.mean(valid_losses))\n",
    "\n",
    "        batch_iter.close()\n",
    "view rawtrainer.py hosted with ❤ by GitHub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
